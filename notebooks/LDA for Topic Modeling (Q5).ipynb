{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "app = pd.read_pickle('app_cleaned.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'category', u'current_rating', u'description', u'id',\n",
       "       u'is_InAppPurcased', u'is_multilingual', u'is_multiplatform', u'name',\n",
       "       u'new_version_desc', u'num_current_rating', u'num_overall_rating',\n",
       "       u'overall_rating', u'price', u'publish_date', u'review1',\n",
       "       u'review1_star', u'review2', u'review2_star', u'review3',\n",
       "       u'review3_star', u'scrape_date', u'seller', u'size', u'update_date',\n",
       "       u'url', u'version'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply use current rating to define the quality of a app. If the current rating is no less than 4.0, it can be seen as a good app. If the current rating is no more than 2.5, it is a bad app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_app = app.loc[app['current_rating'] >=4.0]\n",
    "bad_app = app.loc[app['current_rating'] <=2.5]\n",
    "good_app = good_app.reset_index(drop=True)\n",
    "bad_app = bad_app.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category = app['category']\n",
    "cate_list = []\n",
    "for i in category.unique():\n",
    "    cate = i.lower()\n",
    "    cate_list.append(cate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use star value of different reviews to filter comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_good= good_app.loc[good_app['review1_star']>=4].reset_index(drop=True)['review1']\n",
    "second_good = good_app.loc[good_app['review2_star']>=4].reset_index(drop=True)['review2']\n",
    "third_good = good_app.loc[good_app['review3_star']>=4].reset_index(drop=True)['review3']\n",
    "first_bad = bad_app.loc[bad_app['review1_star']<=2.5].reset_index(drop=True)['review1']\n",
    "second_bad = bad_app.loc[bad_app['review2_star']<=2.5].reset_index(drop=True)['review2']\n",
    "third_bad = bad_app.loc[bad_app['review3_star']<=2.5].reset_index(drop=True)['review3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_rev = first_good.append(second_good)\n",
    "all_good = good_rev.append(third_good)\n",
    "bad_rev = first_bad.append(second_bad)\n",
    "all_bad = bad_rev.append(third_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Cleaning and Preprocessing</b>\n",
    "\n",
    "Data cleaning is absolutely crucial for generating a useful topic model. The steps below are common to most natural language processing methods:\n",
    "   * Tokenizing: converting a document to its atomic elements.\n",
    "   * Stopping: removing meaningless words.\n",
    "   * Stemming: merging words that are equivalent in meaning.\n",
    "\n",
    "Here we need to note that POS tag filter is more about the context of the features than frequencies of features. Topic Modelling tries to map out the recurring patterns of terms into topics. However, every term might not be equally important contextually. For example, POS tag IN contain terms such as – “within”, “upon”, “except”. “CD” contains – “one”,”two”, “hundred” etc. “MD” contains “may”, “must” etc. These terms are the supporting words of a language and can be removed by studying their post tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english')+[u'one',u'app',u'it',u'dont',u\"i\",u\"'s\",\"''\",\"``\",u'use',u'used',u'using',u'love',\n",
    "                                      u'would',u'great',u'app.',u'like',u'lot']+ cate_list)\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def stem(tokens,stemmer = PorterStemmer().stem):\n",
    "    return [stemmer(w.lower()) for w in tokens if w not in stop]\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    tokenize = nltk.word_tokenize\n",
    "    to_token = stem(tokenize(normalized))\n",
    "    tags = nltk.pos_tag(to_token)\n",
    "    dt_tags = [t for t in tags if t[1] in [\"DT\", \"MD\", \"VBP\",\"IN\", \"JJ\",\"VB\"]]\n",
    "    for tag in dt_tags:\n",
    "        normalized = \" \".join(tok for tok in to_token if tok not in tag[0])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_clean_g1 = [clean(doc).split() for doc in first_good]\n",
    "doc_clean_g2 = [clean(doc).split() for doc in second_good]\n",
    "doc_clean_g3 = [clean(doc).split() for doc in third_good]\n",
    "doc_clean_b1 = [clean(doc).split() for doc in first_bad]\n",
    "doc_clean_b2 = [clean(doc).split() for doc in second_bad]\n",
    "doc_clean_b3 = [clean(doc).split() for doc in third_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_clean_good = [clean(doc).split() for doc in all_good]\n",
    "doc_clean_bad = [clean(doc).split() for doc in all_bad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Preparing Document-Term Matrix</b>\n",
    "* Convert a corpus into a document-term matrix. LDA model looks for repeating term patterns in the entire DT matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "dictionary_g1 = corpora.Dictionary(doc_clean_g1)\n",
    "dictionary_g2 = corpora.Dictionary(doc_clean_g2)\n",
    "dictionary_g3 = corpora.Dictionary(doc_clean_g3)\n",
    "dictionary_b1 = corpora.Dictionary(doc_clean_b1)\n",
    "dictionary_b2 = corpora.Dictionary(doc_clean_b2)\n",
    "dictionary_b3 = corpora.Dictionary(doc_clean_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_good = corpora.Dictionary(doc_clean_good)\n",
    "dictionary_bad = corpora.Dictionary(doc_clean_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix_g1 = [dictionary_g1.doc2bow(doc) for doc in doc_clean_g1]\n",
    "doc_term_matrix_g2 = [dictionary_g2.doc2bow(doc) for doc in doc_clean_g2]\n",
    "doc_term_matrix_g3 = [dictionary_g3.doc2bow(doc) for doc in doc_clean_g3]\n",
    "doc_term_matrix_b1 = [dictionary_b1.doc2bow(doc) for doc in doc_clean_b1]\n",
    "doc_term_matrix_b2 = [dictionary_b2.doc2bow(doc) for doc in doc_clean_b2]\n",
    "doc_term_matrix_b3 = [dictionary_b3.doc2bow(doc) for doc in doc_clean_b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_term_matrix_good = [dictionary_good.doc2bow(doc) for doc in doc_clean_good]\n",
    "doc_term_matrix_bad = [dictionary_bad.doc2bow(doc) for doc in doc_clean_bad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Running LDA Model (Batch Wise LDA)</b>\n",
    "   * According to the [reference](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/), in order to retrieve most important topic terms, a corpus can be divided into batches of fixed sizes. Running LDA multiple times on these batches will provide different results, however, the best topic terms will be the intersection of all batches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel_g1 = Lda(doc_term_matrix_g1, num_topics=3, id2word = dictionary_g1, passes=50)\n",
    "ldamodel_g2 = Lda(doc_term_matrix_g2, num_topics=3, id2word = dictionary_g2, passes=50)\n",
    "ldamodel_g3 = Lda(doc_term_matrix_g3, num_topics=3, id2word = dictionary_g3, passes=50)\n",
    "ldamodel_b1 = Lda(doc_term_matrix_b1, num_topics=3, id2word = dictionary_b1, passes=50)\n",
    "ldamodel_b2 = Lda(doc_term_matrix_b2, num_topics=3, id2word = dictionary_b2, passes=50)\n",
    "ldamodel_b3 = Lda(doc_term_matrix_b3, num_topics=3, id2word = dictionary_b3, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Examining the results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.011*year + 0.010*ive + 0.009*best + 0.008*im + 0.008*time'), (1, u'0.011*time + 0.007*want + 0.007*realli + 0.007*get + 0.006*also'), (2, u'0.022*get + 0.011*check + 0.011*need + 0.011*forecast + 0.009*hope')]\n",
      "[(0, u'0.011*get + 0.010*game + 0.009*time + 0.008*realli + 0.007*help'), (1, u'0.010*time + 0.008*easi + 0.008*make + 0.007*work + 0.007*get'), (2, u'0.006*time + 0.006*year + 0.006*day + 0.006*work + 0.005*go')]\n",
      "[(0, u'0.014*free + 0.013*call + 0.011*phone + 0.010*make + 0.010*work'), (1, u'0.033*screen + 0.030*know + 0.019*go + 0.018*everi + 0.017*see'), (2, u'0.022*right + 0.018*also + 0.017*calcul + 0.013*away + 0.013*need')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel_g1.print_topics(num_topics=3, num_words=5))\n",
    "print(ldamodel_g2.print_topics(num_topics=3, num_words=5))\n",
    "print(ldamodel_g3.print_topics(num_topics=3, num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each generated topic is separated by a comma. Within each topic are the five most probable words to appear in that topic. The best topic terms will be the intersection of all three batches. Some things to think about, for the good app, the comments have common features like: \n",
    "  1. It's free have some good features that satisfy customers' demand.\n",
    "  2. It has many good information and details, and customers are comfortable at vision, like screen.\n",
    "  3. The speed is awesome and save some time.\n",
    "  4. It provids some help when customers using it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.010*time + 0.010*get + 0.009*work + 0.009*even + 0.007*tri'), (1, u'0.011*time + 0.007*updat + 0.007*work + 0.007*get + 0.006*im'), (2, u'0.011*time + 0.007*tri + 0.006*phone + 0.006*go + 0.006*get')]\n",
      "[(0, u'0.012*get + 0.008*work + 0.008*tri + 0.008*time + 0.007*im'), (1, u'0.014*time + 0.012*get + 0.009*work + 0.007*need + 0.007*even'), (2, u'0.011*time + 0.009*work + 0.009*version + 0.007*tri + 0.007*updat')]\n",
      "[(0, u'0.011*updat + 0.010*back + 0.009*work + 0.007*read + 0.006*tri'), (1, u'0.008*get + 0.008*work + 0.007*ad + 0.006*time + 0.006*doesnt'), (2, u'0.018*time + 0.011*get + 0.009*even + 0.008*tri + 0.007*work')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel_b1.print_topics(num_topics=3, num_words=5))\n",
    "print(ldamodel_b2.print_topics(num_topics=3, num_words=5))\n",
    "print(ldamodel_b3.print_topics(num_topics=3, num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the bad apps, from the result, we can see most topics include the word \"time\". We can refer that customers are not satisfied for the using fluency of these apps. And for the updated version of these apps, they doesn't work sometimes, maybe because flashing back. Meanwhile, compared with the last version, these updated apps maybe designed not that good. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Running LDA Model (For the whole documents)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel_good = Lda(doc_term_matrix_good, num_topics=10, id2word = dictionary_good, passes=20)\n",
    "ldamodel_bad = Lda(doc_term_matrix_bad, num_topics=10, id2word = dictionary_bad, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, u'0.040*open + 0.029*see + 0.027*screen'), (2, u'0.013*time + 0.011*im + 0.011*get'), (9, u'0.030*accur + 0.027*also + 0.025*time'), (7, u'0.026*day + 0.020*everi + 0.014*help'), (3, u'0.021*inform + 0.020*locat + 0.020*map')]\n",
      "[(6, u'0.013*time + 0.011*work + 0.010*even'), (9, u'0.014*work + 0.013*time + 0.009*doesnt'), (0, u'0.013*get + 0.012*work + 0.012*time'), (5, u'0.012*page + 0.011*time + 0.010*get'), (2, u'0.016*time + 0.009*need + 0.007*show')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel_good.print_topics(num_topics=5, num_words=3))\n",
    "print(ldamodel_bad.print_topics(num_topics=5, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "good_rev = pyLDAvis.gensim.prepare(ldamodel_good, doc_term_matrix_good, dictionary_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
